{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树\n",
    "\n",
    "**决策树（decision tree）**是一个树结构（可以是二叉树或非二叉树）。其**每个非叶节点表示一个特征属性上的测试**，**每个分支代表这个特征属性在某个值域上的输出**，**每个叶节点存放一个决策结果**。使用决策树进行决策的过程就是从根节点开始，按照某些规则选择一个属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果。\n",
    "\n",
    "构建决策树最主要的两个问题是：如何**分裂属性**与**选择哪个属性分裂**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 如何分裂属性\n",
    "**构造决策树的关键步骤是分裂属性**。所谓分裂属性就是在某个节点处按照某一特征属性的不同划分构造不同的分支，其目标是让各个分裂子集尽可能地“纯”。分裂属性分为三种不同的情况：\n",
    "\n",
    "+ 属性是离散值且不要求生成二叉决策树。此时用属性的每一个划分作为一个分支。\n",
    "+ 属性是离散值且要求生成二叉决策树。此时使用属性划分的一个子集进行测试，按照“属于此子集”和“不属于此子集”分成两个分支。\n",
    "+ 属性是连续值。此时确定一个值作为分裂点split_point，按照>split_point和<=split_point生成两个分支。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 选择哪个属性分裂\n",
    "\n",
    "**构造决策树的关键性内容是进行属性选择度量，即：选择哪一个属性来分裂**。属性选择度量是一种选择分裂准则，是将给定的类标记的训练集合的数据划分“最好”地分成个体类的启发式方法。**常见的决策树算法有ID3，C4.5和CART，其中ID3和C4.5用于分类，CART可用于分类与回归**。\n",
    "\n",
    "对于决策树的**分类**即：**选择叶节点属于的最多的类为最终类。**\n",
    "\n",
    "对于决策树的**回归**即：**选择叶节点属于的值的平均值作为回归值。**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 ID3算法\n",
    "\n",
    "算法的核心思想就是以**信息增益**度量属性选择，选择分裂后信息增益最大的属性进行分裂。\n",
    "\n",
    "下面使用一个SNS社区中不真实账号检测的例子来说明决策树的构造，训练集如下（其中s,m,l分别表示小，中，大）：\n",
    "\n",
    "![dicision_tree_id3_example.png](./resources/dicision_tree_id3_example.png)\n",
    "\n",
    "用L，F，H分别表示日志密度，好友密度，是否使用真实头像等3个特征属性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 信息增益计算\n",
    "\n",
    "1.**计算分类类别对训练集$D$划分的熵**：$info(D) = - \\sum_{i=1}^{m}p_ilog_2(p_i)$（其中$p_i$表示第i个类别在训练集中出现的概率,实际计算可用频率替代），这个$info(D)$值越小，说明样本集合D的纯度就越高。\n",
    "\n",
    "对于例子的计算方式如下：$info(D) = -0.7log_20.7 - 0.3 log_20.3 = 0.879$\n",
    "\n",
    "2.**计算属性$A$对$D$划分的期望信息**：$info_A(D) = \\sum_{j=1}^v \\dfrac{|D_j|}{|D|}info(D_j)$（其中$j$代表属性$A$的某一类取值)。\n",
    "\n",
    "对于例子的计算方式如下：$info_L(D)=\\dfrac{3}{10}\\times(-\\dfrac{1}{3}log_2\\dfrac{1}{3} - \\dfrac{2}{3}log_2\\dfrac{2}{3}) + \\dfrac{4}{10}\\times(-\\dfrac{1}{4}log_2\\dfrac{1}{4}-\\dfrac{3}{4}log_2\\dfrac{3}{4}) + \\dfrac{3}{10}\\times(-\\dfrac{0}{3}log_2\\dfrac{0}{3}-\\dfrac{3}{3}log_2\\dfrac{3}{3})=0.603$\n",
    "\n",
    "3.**计算信息增益**：$gain(L) = 0.879 - 0.603=0.276$，因此选择日志密度的属性增益为0.276。\n",
    "\n",
    "用同样方法得到H和F的信息增益分别为0.033和0.553。因为F具有最大的信息增益（划分后纯度提升越高），所以第一次分裂选择F为分裂属性，分类结果如下：\n",
    "\n",
    "![dicision_tree_id3_example1.png](./resources/dicision_tree_id3_example1.png)\n",
    "\n",
    "在上图的基础上，再递归使用这个方法计算子节点的分裂属性，最终就可以得到整个决策树。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 C4.5算法\n",
    "\n",
    "C4.5决策树的提出是为了解决ID3决策树的一个缺点：**当一个属性的可取值数目较多时，那么每个可取值对应的样本可能只有一个或者是很少个，那么这个时候它的信息增益是非常高的**，ID3决策树会认为这个属性很适合划分，但是较多取值的属性来进行划分带来的问题是它的泛化能力比较弱，不能够对新样本进行有效的预测。\n",
    "\n",
    "所以C4.5决策树则不直接使用信息增益来作为划分样本的主要依据，而使用增益率，其**对可取值数目较少的属性有所偏好**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 CART算法\n",
    "\n",
    "**CART全称为Classification and Regression Tree，其可以用于回归于分类。** 其通过**基尼指数**选择最优特征，同时决定该特征的**最优二值切分点**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考资料\n",
    "\n",
    "1. [分类算法--决策树(Decision tree)](http://lib.csdn.net/article/machinelearning/39666)\n",
    "2. [ID3、C4.5、CART三种决策树的区别](https://blog.csdn.net/qq_27717921/article/details/74784400)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
