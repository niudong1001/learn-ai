{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL Auxiliary\n",
    "\n",
    "使用深度学习中涉及的一些辅助知识。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.InteractiveSession()\n",
    "init=tf.global_variables_initializer()\n",
    "init.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "dropout是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃。注意是暂时，对于随机梯度下降来说，由于是随机丢弃，故而每一个mini-batch都在训练不同的网络。dropout是CNN中防止过拟合提高效果的一个大杀器。\n",
    "\n",
    "关于为什么dropout可以有很好的效果，有两种观点：\n",
    "\n",
    "**组合派**：Hinton的观点，大规模的神经网络有两个缺点：**费时**；**容易过拟合**。一般解决过拟合会训练多个模型来组合获得更好的效果，但这又会落入费时的问题里面。而使用dropout，每次相当于找到了一个更瘦的网络，这样就在不增加网络参数的情况下，训练了很多不同结构的网络，这样就缓解了过拟合问题。\n",
    "\n",
    "**动机论**：在自然选择中，选择有性繁殖而不是无性繁殖说明了**基因的力量在于混合的能力而非单个基因的能力**。dropout也能达到同样的效果，它强迫一个神经单元，和随机挑选出来的其他神经单元共同工作，达到好的效果。**减弱了神经元节点间的联合适应性，增强了泛化能力**。\n",
    "\n",
    "### 参考文档：\n",
    "\n",
    "- [CNN中的dropout理解](https://blog.csdn.net/dod_jdi/article/details/78379781)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Softmax\n",
    "\n",
    "即对于一张图片每个通道中所有像素进行softmax分类，如下所示代码：\n",
    "\n",
    "```python\n",
    "# Assume features is of size [N, H, W, C] (batch_size, height, width, channels).\n",
    "# Transpose it to [N, C, H, W], then reshape to [N * C, H * W] to compute softmax\n",
    "# jointly over the image dimensions. \n",
    "features = tf.reshape(tf.transpose(features, [0, 3, 1, 2]), [N * C, H * W])\n",
    "softmax = tf.nn.softmax(features)\n",
    "# Reshape and transpose back to original format.\n",
    "softmax = tf.transpose(tf.reshape(softmax, [N, C, H, W]), [0, 2, 3, 1])\n",
    "```\n",
    "\n",
    "### 参考文档：\n",
    "\n",
    "- [Request for spatial softmax implementation](https://github.com/tensorflow/tensorflow/issues/6271)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xavier Init\n",
    "\n",
    "一种权值初始化方法，它保证了前向传播和反向传播时每一层的方差一致，根据**每层的输入个数$n_{in}$和输出个数$n_{out}$来决定参数随机初始化的分布范围**：\n",
    "\n",
    "> $[-\\dfrac{\\sqrt{6}}{\\sqrt{n_{in}+n_{out}}},\\dfrac{\\sqrt{6}}{\\sqrt{n_{in}+n_{out}}}]$\n",
    "\n",
    "Python的实现代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.6043839  -0.6561047 ]\n",
      " [-0.27995718  0.4229511 ]]\n"
     ]
    }
   ],
   "source": [
    "def xavier_init(fan_in, fan_out, constant = 1):\n",
    "    low = -constant * np.sqrt(6.0 / (fan_in + fan_out))\n",
    "    high = constant * np.sqrt(6.0 / (fan_in + fan_out))\n",
    "    return tf.random_uniform((fan_in, fan_out),\n",
    "                             minval=low, maxval=high, dtype=tf.float32)\n",
    "print(xavier_init(2, 2).eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考文档：\n",
    "\n",
    "- [Understanding the difficulty of training deep feedforward neural networks](https://www.researchgate.net/publication/215616968_Understanding_the_difficulty_of_training_deep_feedforward_neural_networks)\n",
    "- [参数随机初始化方法：xavier_init()](https://blog.csdn.net/nini_coded/article/details/79302820)\n",
    "- [自编码器参数初始化方法-Xavier initialization](https://www.jianshu.com/p/4e53d3c604f6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
